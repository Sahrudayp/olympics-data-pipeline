# olympics-data-pipeline
"End-to-end ETL pipeline using PySpark and AWS S3 for Tokyo 2020 Olympics data."
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Sahrudayp/olympics-data-pipeline/blob/main/notebooks/Olympics_ETL.ipynb)
#  Project Architecture
![Architecture Diagram](images/Architecture.png)
![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![Apache Spark](https://img.shields.io/badge/Apache%20Spark-E25A1C?style=for-the-badge&logo=apachespark&logoColor=white)
![AWS](https://img.shields.io/badge/AWS-%23FF9900.svg?style=for-the-badge&logo=amazon-aws&logoColor=white)
## üõ†Ô∏è Tech Stack & Tools

| Category | Tools | Use Case |
| :--- | :--- | :--- |
| **Language** | Python 3.9+ | Data ingestion and pipeline scripting |
| **Processing** | Apache Spark (PySpark) | Distributed data cleaning and transformation |
| **Cloud Storage** | AWS S3 | 3-Tier Data Lake (Bronze, Silver, Gold) |
| **Infrastructure** | Boto3 (AWS SDK) | Programmatic access to AWS resources |
| **Visualization** | Matplotlib / Seaborn | Generating business insights and charts |
| **Environment** | Google Colab | Cloud-based Spark development environment |
| **Version Control**| Git / GitHub | Project management and documentation |
